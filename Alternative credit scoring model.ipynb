{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative credit scoring\n",
    "\n",
    "Tracking the informational gain of alternative data is an important step to make alternative credit scoring approaches viable. This notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABLCAYAAABHlv13AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAAhNJREFUeJzt2zFqVFEUx+Fzk4ljQJHIpLDSqZ1CMBsQS/cxnRvKmrQTC1vb7MDiWmhj40sIl/efy/d1A6c4B4ZfceG13nsBsL6ztRcA4A9BBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJAiM3SQGvtWFXHqqqz8837y+cvhi+1mu3TtTcYqm0v115hmM3F4l/5pG3P5/6i9slm7vt+fv9x13u/XpprD/l0+tnVrh8+fnrUYsk2+8PaKwx1sX+79grDvHy1W3uFod5c/Vp7haH2k9/3+d2Hr733m6U5TxYAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQggwQQpABQggyQAhBBgghyAAhBBkghCADhBBkgBCCDBBCkAFCCDJACEEGCCHIACEEGSCEIAOEEGSAEIIMEEKQAUIIMkAIQQYIIcgAIQQZIIQgA4QQZIAQrff+/4HWjlV1/PvzUFXfRi+1ol1V3a29xCAz31blvlM3+32ve+/XS0OLQf5nuLUvvfebR60VbOb7Zr6tyn2nbvb77suTBUAIQQYI8dAg3w7ZIsfM9818W5X7Tt3s993Lg96QARjHkwVACEEGCCHIACEEGSCEIAOE+A1oRUkhQuXHhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Imports and settings\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "sns.palplot(sns.color_palette(\"GnBu_d\"))\n",
    "sns.set(rc={'figure.figsize':(20,7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data on applicant records (ar), corresponding credit records (cr), and dictionary (d)\n",
    "df_ar = pd.read_csv('Data/application_record.csv')\n",
    "df_cr = pd.read_csv('Data/credit_record.csv')\n",
    "df_d = pd.read_excel('Data/dictionary.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Feature name</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>application_record</td>\n",
       "      <td>ID</td>\n",
       "      <td>client number</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'CODE_GENDER'</td>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAGOWNCAR'</td>\n",
       "      <td>Is there a car</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAGOWNREALTY'</td>\n",
       "      <td>Is there a property</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'CNT_CHILDREN'</td>\n",
       "      <td>Number of children</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'AMTINCOMETOTAL'</td>\n",
       "      <td>Annual income</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'NAMEINCOMETYPE'</td>\n",
       "      <td>Income category</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'NAMEEDUCATIONTYPE'</td>\n",
       "      <td>education level</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'NAMEFAMILYSTATUS'</td>\n",
       "      <td>Marital status</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'NAMEHOUSINGTYPE'</td>\n",
       "      <td>Way of living</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'DAYS_BIRTH'</td>\n",
       "      <td>birthday</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'DAYS_EMPLOYED'</td>\n",
       "      <td>Start date</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAG_MOBIL'</td>\n",
       "      <td>Is there a mobile phone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAGWORKPHONE'</td>\n",
       "      <td>Is there a work phone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAG_PHONE'</td>\n",
       "      <td>Is there a phone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'FLAG_EMAIL'</td>\n",
       "      <td>Is there an email</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'OCCUPATION_TYPE'</td>\n",
       "      <td>Occupation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>application_record</td>\n",
       "      <td>'CNTFAMMEMBERS'</td>\n",
       "      <td>Family size</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>credit_record</td>\n",
       "      <td>ID</td>\n",
       "      <td>client number</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>credit_record</td>\n",
       "      <td>MONTHS_BALANCE</td>\n",
       "      <td>record month</td>\n",
       "      <td>The month of the extracted data is the startin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>credit_record</td>\n",
       "      <td>STATUS</td>\n",
       "      <td>status</td>\n",
       "      <td>0: 1-29 days past due 1: 30-59 days past due 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Type         Feature name              Explanation  \\\n",
       "0   application_record                   ID            client number   \n",
       "1   application_record        'CODE_GENDER'                   gender   \n",
       "2   application_record         'FLAGOWNCAR'           Is there a car   \n",
       "3   application_record      'FLAGOWNREALTY'      Is there a property   \n",
       "4   application_record       'CNT_CHILDREN'       Number of children   \n",
       "5   application_record     'AMTINCOMETOTAL'            Annual income   \n",
       "6   application_record     'NAMEINCOMETYPE'          Income category   \n",
       "7   application_record  'NAMEEDUCATIONTYPE'          education level   \n",
       "8   application_record   'NAMEFAMILYSTATUS'           Marital status   \n",
       "9   application_record    'NAMEHOUSINGTYPE'            Way of living   \n",
       "10  application_record         'DAYS_BIRTH'                 birthday   \n",
       "11  application_record      'DAYS_EMPLOYED'               Start date   \n",
       "12  application_record         'FLAG_MOBIL'  Is there a mobile phone   \n",
       "13  application_record      'FLAGWORKPHONE'    Is there a work phone   \n",
       "14  application_record         'FLAG_PHONE'         Is there a phone   \n",
       "15  application_record         'FLAG_EMAIL'        Is there an email   \n",
       "16  application_record    'OCCUPATION_TYPE'               Occupation   \n",
       "17  application_record      'CNTFAMMEMBERS'              Family size   \n",
       "18       credit_record                   ID            client number   \n",
       "19       credit_record       MONTHS_BALANCE             record month   \n",
       "20       credit_record               STATUS                   status   \n",
       "\n",
       "                                              Remarks  \n",
       "0                                                   0  \n",
       "1                                                   0  \n",
       "2                                                   0  \n",
       "3                                                   0  \n",
       "4                                                   0  \n",
       "5                                                   0  \n",
       "6                                                   0  \n",
       "7                                                   0  \n",
       "8                                                   0  \n",
       "9                                                   0  \n",
       "10                                                  0  \n",
       "11                                                  0  \n",
       "12                                                  0  \n",
       "13                                                  0  \n",
       "14                                                  0  \n",
       "15                                                  0  \n",
       "16                                                  0  \n",
       "17                                                  0  \n",
       "18                                                  0  \n",
       "19  The month of the extracted data is the startin...  \n",
       "20  0: 1-29 days past due 1: 30-59 days past due 2...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Review what variables mean based on dictionary\n",
    "df_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0: 1-29 days past due 1: 30-59 days past due 2: 60-89 days overdue 3: 90-119 days overdue 4: 120-149 days overdue 5: Overdue or bad debts write-offs for more than 150 days C: paid off that month X: No loan for the month'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Review what credit record status corresponds to a default -> a '5' corresponds to default\n",
    "df_d.loc[df_d['Feature name'] == 'STATUS']['Remarks'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of people with loan information is 45985.\n",
      "The number of people who defaulted on their loan is 195.\n",
      "The share of people who defaulted on their loan is 0.004240513210829618.\n"
     ]
    }
   ],
   "source": [
    "#Now we know that a '5' corresponds to a severely overdue credit or a bed debt write-off. \n",
    "#We can now engineer this feature and find out how many people defaulted on their products\n",
    "\n",
    "df_cr['Count'] = [1 if x != 'X' else 0 for x in df_cr['STATUS']] #only count active loans\n",
    "\n",
    "df_cr['Defaulted'] = [1 if x == '5' else 0 for x in df_cr['STATUS']]\n",
    "\n",
    "df_cr_aggregated = df_cr.groupby('ID', as_index = False).sum()\n",
    "\n",
    "default_rate = len(df_cr_aggregated.loc[df_cr_aggregated['Defaulted'] != 0]) / len(df_cr_aggregated)\n",
    "\n",
    "print('The number of people with loan information is {}.'.format(len(df_cr_aggregated)))\n",
    "print('The number of people who defaulted on their loan is {}.'.format(len(df_cr_aggregated.loc[df_cr_aggregated['Defaulted'] != 0])))\n",
    "print('The share of people who defaulted on their loan is {}.'.format(default_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of customers who defaulted is quite small (0.4%) so I will create a subset that consists 10% of those customers and 90% of customers who successfully repaid their loans to simulate the real world. The 90% will be selected at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MONTHS_BALANCE</th>\n",
       "      <th>Count</th>\n",
       "      <th>Defaulted</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>NAME_INCOME_TYPE</th>\n",
       "      <th>NAME_EDUCATION_TYPE</th>\n",
       "      <th>NAME_FAMILY_STATUS</th>\n",
       "      <th>NAME_HOUSING_TYPE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>FLAG_MOBIL</th>\n",
       "      <th>FLAG_WORK_PHONE</th>\n",
       "      <th>FLAG_PHONE</th>\n",
       "      <th>FLAG_EMAIL</th>\n",
       "      <th>OCCUPATION_TYPE</th>\n",
       "      <th>CNT_FAM_MEMBERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5022003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-9952</td>\n",
       "      <td>-1613</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5021898</td>\n",
       "      <td>-351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>328500.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Civil marriage</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-20940</td>\n",
       "      <td>365243</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5069071</td>\n",
       "      <td>-55</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>State servant</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-18893</td>\n",
       "      <td>-2423</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Core staff</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5068068</td>\n",
       "      <td>-270</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>99000.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-22799</td>\n",
       "      <td>365243</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5045117</td>\n",
       "      <td>-207</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>Pensioner</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Widow</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21126</td>\n",
       "      <td>365243</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5033506</td>\n",
       "      <td>-630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-9883</td>\n",
       "      <td>-780</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sales staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5111116</td>\n",
       "      <td>-315</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-11602</td>\n",
       "      <td>-1455</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Managers</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5089801</td>\n",
       "      <td>-780</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>126000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Separated</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-21646</td>\n",
       "      <td>-1283</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Cleaning staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5045809</td>\n",
       "      <td>-350</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Higher education</td>\n",
       "      <td>Married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-15519</td>\n",
       "      <td>-2026</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5088042</td>\n",
       "      <td>-259</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>216000.0</td>\n",
       "      <td>Working</td>\n",
       "      <td>Secondary / secondary special</td>\n",
       "      <td>Single / not married</td>\n",
       "      <td>House / apartment</td>\n",
       "      <td>-17937</td>\n",
       "      <td>-3054</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Medicine staff</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID  MONTHS_BALANCE  Count  Defaulted CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0  5022003               0      0          0           M            Y   \n",
       "1  5021898            -351      0          0           F            N   \n",
       "2  5069071             -55     11          0           F            Y   \n",
       "3  5068068            -270     15          0           F            N   \n",
       "4  5045117            -207      5          0           F            N   \n",
       "5  5033506            -630      0          0           M            Y   \n",
       "6  5111116            -315     21          0           F            N   \n",
       "7  5089801            -780     20          0           F            N   \n",
       "8  5045809            -350     18          0           M            Y   \n",
       "9  5088042            -259      7          0           F            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL NAME_INCOME_TYPE  \\\n",
       "0               Y             0          112500.0          Working   \n",
       "1               Y             0          328500.0        Pensioner   \n",
       "2               Y             0          157500.0    State servant   \n",
       "3               Y             0           99000.0        Pensioner   \n",
       "4               Y             0          202500.0        Pensioner   \n",
       "5               N             0          157500.0          Working   \n",
       "6               Y             1          315000.0          Working   \n",
       "7               Y             0          126000.0          Working   \n",
       "8               N             0          135000.0          Working   \n",
       "9               Y             0          216000.0          Working   \n",
       "\n",
       "             NAME_EDUCATION_TYPE    NAME_FAMILY_STATUS  NAME_HOUSING_TYPE  \\\n",
       "0  Secondary / secondary special  Single / not married  House / apartment   \n",
       "1  Secondary / secondary special        Civil marriage  House / apartment   \n",
       "2  Secondary / secondary special               Married  House / apartment   \n",
       "3  Secondary / secondary special               Married  House / apartment   \n",
       "4  Secondary / secondary special                 Widow  House / apartment   \n",
       "5  Secondary / secondary special  Single / not married  House / apartment   \n",
       "6               Higher education               Married  House / apartment   \n",
       "7  Secondary / secondary special             Separated  House / apartment   \n",
       "8               Higher education               Married  House / apartment   \n",
       "9  Secondary / secondary special  Single / not married  House / apartment   \n",
       "\n",
       "   DAYS_BIRTH  DAYS_EMPLOYED  FLAG_MOBIL  FLAG_WORK_PHONE  FLAG_PHONE  \\\n",
       "0       -9952          -1613           1                0           0   \n",
       "1      -20940         365243           1                0           0   \n",
       "2      -18893          -2423           1                0           0   \n",
       "3      -22799         365243           1                0           0   \n",
       "4      -21126         365243           1                0           0   \n",
       "5       -9883           -780           1                1           0   \n",
       "6      -11602          -1455           1                0           0   \n",
       "7      -21646          -1283           1                1           1   \n",
       "8      -15519          -2026           1                1           0   \n",
       "9      -17937          -3054           1                0           0   \n",
       "\n",
       "   FLAG_EMAIL OCCUPATION_TYPE  CNT_FAM_MEMBERS  \n",
       "0           0             NaN              1.0  \n",
       "1           0             NaN              2.0  \n",
       "2           0      Core staff              2.0  \n",
       "3           0             NaN              2.0  \n",
       "4           0             NaN              1.0  \n",
       "5           0     Sales staff              1.0  \n",
       "6           0        Managers              3.0  \n",
       "7           0  Cleaning staff              1.0  \n",
       "8           0             NaN              2.0  \n",
       "9           0  Medicine staff              1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create random sample out of non-defaulted applicants and append defaulted applicants\n",
    "df_subset = df_cr_aggregated.loc[df_cr_aggregated['Defaulted']==0].sample(9*195).append(df_cr_aggregated.loc[df_cr_aggregated['Defaulted'] != 0])\n",
    "\n",
    "#Merge that information with applicant data\n",
    "result = pd.merge(df_subset, df_ar, on=['ID'])\n",
    "print(len(result))\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of people with loan information is 1541.\n",
      "The number of people who defaulted on their loan is 180.\n",
      "The share of people who defaulted on their loan is 0.11680726800778715.\n"
     ]
    }
   ],
   "source": [
    "#Some IDs could not be matched, hence we confirm that we still have a roughly 90% / 10% split in the data:\n",
    "print('The number of people with loan information is {}.'.format(len(result)))\n",
    "print('The number of people who defaulted on their loan is {}.'.format(len(result.loc[result['Defaulted'] != 0])))\n",
    "print('The share of people who defaulted on their loan is {}.'.format(len(result.loc[result['Defaulted'] != 0])/len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['defaulted']\n",
      "2\n",
      "['code_gender']\n",
      "2\n",
      "['flag_own_car']\n",
      "2\n",
      "['flag_own_realty']\n",
      "2\n",
      "['cnt_children']\n",
      "5\n",
      "['amt_income_total']\n",
      "107\n",
      "['name_income_type']\n",
      "4\n",
      "['name_education_type']\n",
      "5\n",
      "['name_family_status']\n",
      "5\n",
      "['name_housing_type']\n",
      "6\n",
      "['days_birth']\n",
      "1288\n",
      "['days_employed']\n",
      "943\n",
      "['flag_mobil']\n",
      "1\n",
      "['flag_work_phone']\n",
      "2\n",
      "['flag_phone']\n",
      "2\n",
      "['flag_email']\n",
      "2\n",
      "['occupation_type']\n",
      "18\n",
      "['cnt_fam_members']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#We now get rid of unnecessary columns and make categorical values numeric\n",
    "\n",
    "#Rename and drop irrelevant columns\n",
    "result.columns = [x.lower() for x in result.columns.tolist()] #for easier typing\n",
    "result.drop(labels = ['id', 'months_balance', 'count'], axis = 1, inplace=True)\n",
    "\n",
    "#convert binary answers to numeric variables\n",
    "result['defaulted'] = [1 if x != 0 else 0 for x in result['defaulted'].tolist()] \n",
    "result['code_gender'] = [1 if x == 'M' else 0 for x in result['code_gender'].tolist()] \n",
    "result['flag_own_car'] = [1 if x == 'Y' else 0 for x in result['flag_own_car'].tolist()] \n",
    "result['flag_own_realty'] = [1 if x == 'Y' else 0 for x in result['flag_own_realty'].tolist()] \n",
    "\n",
    "#We check whether there might be some categorical variables with too many possible values.\n",
    "for i in result.columns:\n",
    "    print([i])\n",
    "    print(result[i].nunique())\n",
    "    \n",
    "#Occupation_type has over 18 possible values, which seems a lot for our dataset. It also has over 400 nans, hence drop it.\n",
    "result.drop('occupation_type', axis = 1, inplace=True)\n",
    "\n",
    "#we now get dummies for our remaining categorical vaiables\n",
    "final = pd.get_dummies(result) #we will use xgb so multicolliniarity should not be an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now need to distinguish between 'traditional' and 'hybrid' variables for credit scoring\n",
    "#The hybrid contains all variables to calculate the informational value-add of alternative data\n",
    "traditional = ['defaulted',\n",
    " 'code_gender',\n",
    " 'amt_income_total',\n",
    " 'days_birth',\n",
    " 'days_employed',\n",
    " 'name_income_type_Commercial associate',\n",
    " 'name_income_type_Pensioner',\n",
    " 'name_income_type_State servant',\n",
    " 'name_income_type_Working']\n",
    "hybrid = final.columns.to_list()\n",
    "\n",
    "#we create two datasets to be used for model training to measure the difference in performance\n",
    "final_hybrid = final\n",
    "final_traditional = final[traditional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a general function for exporting data to csv\n",
    "def make_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "    # make data dir, if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    values = np.column_stack((y, x))\n",
    "    \n",
    "    pd.DataFrame(values).to_csv(os.path.join(data_dir, filename), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train (67%), validation (11%), and test sets (22%) for both traditional and hybrid data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#export hybrid data ensuring that label column is first\n",
    "data_dir = 'hybrid_credit_data'\n",
    "X = final_hybrid.drop('defaulted', axis=1)\n",
    "y = final_hybrid['defaulted']\n",
    "X_train_hybrid, X_test_hybrid, y_train_hybrid, y_test_hybrid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "make_csv(y_train_hybrid, X_train_hybrid, filename='train.csv', data_dir=data_dir)\n",
    "make_csv(y_test_hybrid[:round(0.3*len(y_test_hybrid))], X_test_hybrid[:round(0.3*len(X_test_hybrid))], filename='validation.csv', data_dir=data_dir)\n",
    "pd.DataFrame(X_test_hybrid[round(0.3*len(X_test_hybrid)):]).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "#export traditional data ensuring that label column is first\n",
    "data_dir = 'traditional_credit_data'\n",
    "X = final_traditional.drop('defaulted', axis=1)\n",
    "y = final_traditional['defaulted']\n",
    "X_train_traditional, X_test_traditional, y_train_traditional, y_test_traditional = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "make_csv(y_train_traditional, X_train_traditional, filename='train.csv', data_dir=data_dir)\n",
    "make_csv(y_test_traditional[:round(0.3*len(y_test_traditional))], X_test_traditional[:round(0.3*len(X_test_traditional))], filename='validation.csv', data_dir=data_dir)\n",
    "pd.DataFrame(X_test_traditional[round(0.3*len(X_test_traditional)):]).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "#NB: we have already made sure that the labels column (defauled) is first, as required by SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up AWS model infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for SageMaker\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-303249258021/traditional_credit_scoring\n",
      "s3://sagemaker-eu-central-1-303249258021/hybrid_credit_scoring\n"
     ]
    }
   ],
   "source": [
    "#Uplad traditional data to S3\n",
    "data_dir = 'traditional_credit_data'\n",
    "prefix = 'traditional_credit_scoring'\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)\n",
    "\n",
    "#Upload hybrid data to S3\n",
    "data_dir = 'hybrid_credit_data'\n",
    "prefix = 'hybrid_credit_scoring'\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid_credit_scoring/test.csv\n",
      "hybrid_credit_scoring/train.csv\n",
      "hybrid_credit_scoring/validation.csv\n",
      "traditional_credit_scoring/test.csv\n",
      "traditional_credit_scoring/train.csv\n",
      "traditional_credit_scoring/validation.csv\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "#check whether the data has been uploaded successfully\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('Test passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create predictions with traditional model and assess model performance\n",
    "def model_assessment(data_dir, ground_truth):\n",
    "    '''Function assesses model performance against ground truth data\n",
    "        :param data_dir: data dictionary to retrieve predictions\n",
    "        :param ground_truth: labels of test data set \n",
    "        '''\n",
    "    \n",
    "    predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "    predictions = [round(num) for num in predictions.squeeze().values]\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    return accuracy_score(ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and assess traditional model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 13:24:28 Starting - Starting the training job...\n",
      "2020-06-23 13:24:30 Starting - Launching requested ML instances......\n",
      "2020-06-23 13:25:39 Starting - Preparing the instances for training......\n",
      "2020-06-23 13:26:31 Downloading - Downloading input data...\n",
      "2020-06-23 13:27:29 Training - Training image download completed. Training in progress.\n",
      "2020-06-23 13:27:29 Uploading - Uploading generated training model.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:27:24:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:27:24:INFO] File size need to be processed in the node: 0.06mb. Available memory size in the node: 8496.48mb\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:27:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:27:24] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:27:24] 1032x8 matrix with 8256 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:27:24:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:27:24] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:27:24] 153x8 matrix with 1224 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.291667#011validation-error:0.352941\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.281008#011validation-error:0.339869\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.272287#011validation-error:0.300654\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.274225#011validation-error:0.30719\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.281008#011validation-error:0.294118\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.27907#011validation-error:0.300654\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.261628#011validation-error:0.294118\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.256783#011validation-error:0.294118\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.249031#011validation-error:0.287582\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.251938#011validation-error:0.27451\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.239341#011validation-error:0.313726\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.233527#011validation-error:0.294118\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 18 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.232558#011validation-error:0.300654\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.228682#011validation-error:0.300654\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.228682#011validation-error:0.300654\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.221899#011validation-error:0.281046\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.223837#011validation-error:0.281046\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.222868#011validation-error:0.27451\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 8 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.223837#011validation-error:0.281046\u001b[0m\n",
      "\u001b[34m[13:27:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 12 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.224806#011validation-error:0.27451\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.251938#011validation-error:0.27451\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-23 13:27:36 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n"
     ]
    }
   ],
   "source": [
    "#Define path\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "data_dir = 'traditional_credit_data'\n",
    "prefix = 'traditional_credit_scoring'\n",
    "\n",
    "\n",
    "#get xgb algorithm\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'xgboost')\n",
    "\n",
    "#construct xgb estimator\n",
    "xgb = sagemaker.estimator.Estimator(container, \n",
    "                                    role,                                   \n",
    "                                    train_instance_count=1,                 \n",
    "                                    train_instance_type='ml.m4.xlarge',     \n",
    "                                    output_path='s3://{}/{}/output'.format(sagemaker_session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "#set xgb hyperparameters\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)\n",
    "\n",
    "train_location = sagemaker_session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "validation_location = sagemaker_session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "test_location = sagemaker_session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=validation_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:25 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:25 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:25 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:25 +0000] [37] [INFO] Booting worker with pid: 37\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:25 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:26:INFO] Model loaded successfully for worker : 37\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:26:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:26 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:31:26 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:26:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:26:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:31:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-23:13:31:43:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-23:13:31:43:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2020-06-23T13:31:43.744:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deploy traditional model \n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-central-1-303249258021/xgboost-2020-06-23-13-28-10-900/test.csv.out to traditional_credit_data/test.csv.out\n",
      "0.8932584269662921\n"
     ]
    }
   ],
   "source": [
    "#Save data from S3 locally\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "\n",
    "#Assess model\n",
    "print(model_assessment(data_dir=data_dir, ground_truth=y_test_traditional[round(0.3*len(X_test_traditional)):]))\n",
    "\n",
    "#Store predictions\n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions_traditional = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and assess hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-23 13:32:24 Starting - Starting the training job...\n",
      "2020-06-23 13:32:26 Starting - Launching requested ML instances......\n",
      "2020-06-23 13:33:32 Starting - Preparing the instances for training......\n",
      "2020-06-23 13:34:35 Downloading - Downloading input data...\n",
      "2020-06-23 13:35:12 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:35:33:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:35:33:INFO] File size need to be processed in the node: 0.16mb. Available memory size in the node: 8485.25mb\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:35:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:35:33] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:35:33] 1032x32 matrix with 33024 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:35:33:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[13:35:33] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[13:35:33] 153x32 matrix with 4896 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.24031#011validation-error:0.261438\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.238372#011validation-error:0.30719\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.231589#011validation-error:0.254902\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.229651#011validation-error:0.281046\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.221899#011validation-error:0.267974\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 14 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.218992#011validation-error:0.267974\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 20 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.218023#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.216085#011validation-error:0.254902\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.213178#011validation-error:0.267974\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.205426#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.203488#011validation-error:0.228758\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.20155#011validation-error:0.235294\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.198643#011validation-error:0.24183\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.192829#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.189922#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.187984#011validation-error:0.254902\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.187016#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.189922#011validation-error:0.248366\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.187984#011validation-error:0.235294\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.188953#011validation-error:0.24183\u001b[0m\n",
      "\u001b[34m[13:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 20 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.187016#011validation-error:0.24183\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.203488#011validation-error:0.228758\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-23 13:35:45 Uploading - Uploading generated training model\n",
      "2020-06-23 13:35:45 Completed - Training job completed\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n"
     ]
    }
   ],
   "source": [
    "#Define path\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "data_dir = 'hybrid_credit_data'\n",
    "prefix = 'hybrid_credit_scoring'\n",
    "\n",
    "\n",
    "#get xgb algorithm\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'xgboost')\n",
    "\n",
    "#construct xgb estimator\n",
    "xgb = sagemaker.estimator.Estimator(container, \n",
    "                                    role,                                   \n",
    "                                    train_instance_count=1,                 \n",
    "                                    train_instance_type='ml.m4.xlarge',     \n",
    "                                    output_path='s3://{}/{}/output'.format(sagemaker_session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "#set xgb hyperparameters\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)\n",
    "\n",
    "train_location = sagemaker_session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "validation_location = sagemaker_session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\n",
    "test_location = sagemaker_session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=validation_location, content_type='csv')\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [38] [INFO] Booting worker with pid: 38\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:13:INFO] Model loaded successfully for worker : 38\u001b[0m\n",
      "\u001b[34m[2020-06-23 13:39:13 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:13:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:13:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:13:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-23:13:39:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2020-06-23:13:39:47:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-23:13:39:47:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2020-06-23T13:39:46.919:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deploy hybrid model\n",
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-central-1-303249258021/xgboost-2020-06-23-13-36-06-394/test.csv.out to hybrid_credit_data/test.csv.out\n",
      "0.6235955056179775\n"
     ]
    }
   ],
   "source": [
    "#Save data from S3 locally\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "\n",
    "#Assess model\n",
    "print(model_assessment(data_dir=data_dir, ground_truth=y_test_hybrid[round(0.3*len(y_test_hybrid)):]))\n",
    "\n",
    "#Store predictions\n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions_hybrid = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models and calculate value-add of alternative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to research by Oliver Wyman, a leading management consulting company, in their paper on [‘Alternative Data and the Unbanked’](https://www.oliverwyman.com/content/dam/oliver-wyman/v2/publications/2017/may/Alternative_Data_And_The_Unbanked.pdf) the cut-off rate for obtaining loans currently is at around 4% bad-rate. If we think that a ‘positive’ is someone who is lendable and a ‘negative’ is someone who is non-lendable, the banks essentially accept a false positive rate of 4%. Hence we can formulate the following optimization problem: maximize the number of true positives subject to the false postive rate being below 4%. \n",
    "\n",
    "However, we run into the issue that the traditional model is 'cheating'. Checking its predictions we realize that it takes advantage of the unbalanced dataset and maximizes accuracy by simply predicting that every applicant will not default, thereby reaching an accuracy of around 90% (i.e. the share of applicants who don't default). With this is does better than the hybrid model, but in practice it would be useless for a bank, since it does not help to distinguish between good and bad applicants. So we can note here a first if tentative win for the alternative data model - in some cases alternative data will be the only way to make any statistical inference about applicants if the availability of traditional data is sufficiently constrained.\n",
    "\n",
    "Having reviewed [AWS xgb documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst) to find another eval_metric (default is accuracy for binary classification) that would have the algorithm focus more on the defaulted applicants, I couldn't find anything that would make the xgb based on traditional data deviate from its 'cheating' strategy. Hence, I now balance the dataset 50/50 to compare performance between the traditional and hybrid model and do the ROC analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people predicted to default based on traditional data is 0\n",
      "Number of people predicted to default based on hybrid data is 122\n"
     ]
    }
   ],
   "source": [
    "print('Number of people predicted to default based on traditional data is '+ str(predictions_traditional.count(1)))\n",
    "print('Number of people predicted to default based on hybrid data is '+ str(predictions_hybrid.count(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create balanced dataset\n",
    "final_balanced = final.loc[final['defaulted']==0].sample(195).append(final.loc[final['defaulted'] != 0])\n",
    "\n",
    "#devide into hybrid and traditional data\n",
    "final_hybrid = final_balanced\n",
    "final_traditional = final_balanced[traditional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train (70%) and test sets (30%) for both traditional and hybrid data\n",
    "\n",
    "#export hybrid data ensuring that label column is first\n",
    "data_dir = 'hybrid_credit_data_balanced'\n",
    "X = final_hybrid.drop('defaulted', axis=1)\n",
    "y = final_hybrid['defaulted']\n",
    "X_train_hybrid, X_test_hybrid, y_train_hybrid, y_test_hybrid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#export traditional data ensuring that label column is first\n",
    "data_dir = 'traditional_credit_data_balanced'\n",
    "X = final_traditional.drop('defaulted', axis=1)\n",
    "y = final_traditional['defaulted']\n",
    "X_train_traditional, X_test_traditional, y_train_traditional, y_test_traditional = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#NB: we have already made sure that the labels column (defauled) is first, as required by SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67        62\n",
      "           1       0.60      0.61      0.60        51\n",
      "\n",
      "   micro avg       0.64      0.64      0.64       113\n",
      "   macro avg       0.63      0.63      0.63       113\n",
      "weighted avg       0.64      0.64      0.64       113\n",
      "\n",
      "[[41 21]\n",
      " [20 31]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Test hybrid model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "dtree = RandomForestClassifier(random_state=0)\n",
    "dtree.fit(X_train_hybrid, y_train_hybrid)\n",
    "predictions = dtree.predict(X_test_hybrid)\n",
    "\n",
    "#Import model evaluation metrics and print evaluation results\n",
    "print(classification_report(y_test_hybrid,predictions))\n",
    "print(confusion_matrix(y_test_hybrid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.66      0.65        62\n",
      "           1       0.57      0.55      0.56        51\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       113\n",
      "   macro avg       0.61      0.61      0.61       113\n",
      "weighted avg       0.61      0.61      0.61       113\n",
      "\n",
      "[[41 21]\n",
      " [23 28]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#test traditional model\n",
    "dtree = RandomForestClassifier(random_state=0)\n",
    "dtree.fit(X_train_traditional, y_train_traditional)\n",
    "predictions = dtree.predict(X_test_traditional)\n",
    "\n",
    "#Import model evaluation metrics and print evaluation results\n",
    "print(classification_report(y_test_traditional,predictions))\n",
    "print(confusion_matrix(y_test_traditional,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
